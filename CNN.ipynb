{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU/b2nputNK2XRHnUROHHj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanusha583/Deep_learning/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **What is a Convolutional Neural Network (CNN)?**\n",
        "\n",
        "A CNN is a type of artificial neural network designed to process and analyze visual data, like images. It is inspired by the way the human brain processes visual information in the visual cortex.\n",
        "\n",
        "In the human brain, neurons in the visual cortex are specialized to detect specific patterns, such as edges, shapes, or textures. Similarly, a CNN identifies features in images through layers of connected neurons, progressively learning more complex features.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Steps in a CNN:**\n",
        "\n",
        "1. **Input Image:**\n",
        "   - For example, we have a grayscale image of size $6 \\times 6$, where each pixel intensity value ranges from 0 to 255.\n",
        "   - To normalize the pixel values, we scale them to a range of 0 to 1 using min-max scaling:\n",
        "   $$\n",
        "   \\text{Normalized Value} = \\frac{\\text{Pixel Value}}{255}.\n",
        "   $$\n",
        "\n",
        "2. **Convolution Operation:**\n",
        "   - A **filter (or kernel)** is a small matrix (e.g., $3 \\times 3$) that slides over the image and performs element-wise multiplication followed by summation. This operation helps detect features like edges.\n",
        "   - If the input image is $n \\times n$ (e.g., $6 \\times 6$) and the filter is $f \\times f$ (e.g., $3 \\times 3$), the output size is determined by the formula:\n",
        "   $$\n",
        "   \\text{Output Size} = (n - f + 1) \\times (n - f + 1).\n",
        "   $$\n",
        "\n",
        "   - For a $6 \\times 6$ input and a $3 \\times 3$ filter, the output size becomes $4 \\times 4$.\n",
        "\n",
        "3. **Padding:**\n",
        "   - Padding involves adding a border (of zeros or other values) around the input image to control the size of the output.\n",
        "   - Without padding, the image shrinks after each convolution. Padding prevents this loss of information.\n",
        "   - The formula for output size with padding is:\n",
        "   $$\n",
        "   \\text{Output Size} = (n + 2p - f + 1) \\times (n + 2p - f + 1),\n",
        "   $$\n",
        "   where $p$ is the padding size.\n",
        "\n",
        "   - For example, with $p = 1$, the $6 \\times 6$ image padded becomes $8 \\times 8$, and with a $3 \\times 3$ filter, the output size is $6 \\times 6$ (no shrinkage).\n",
        "\n",
        "4. **Types of Padding:**\n",
        "   - **Valid Padding:** No padding; results in smaller output.\n",
        "   - **Same Padding:** Pads the image so the output size matches the input size.\n",
        "\n",
        "5. **Feature Extraction:**\n",
        "   - The convolutional layer extracts features, such as edges, corners, or textures, by applying multiple filters.\n",
        "\n",
        "6. **Activation Function:**\n",
        "   - After convolution, an activation function (like ReLU) introduces non-linearity, enabling the network to learn complex patterns.\n",
        "\n",
        "7. **Pooling:**\n",
        "   - Pooling layers reduce the size of the feature maps, preserving important features while reducing computational complexity.\n",
        "\n",
        "---\n",
        "\n",
        "### **Relation to the Human Brain:**\n",
        "- Similar to how the **visual cortex** processes images through layers of neurons, a CNN processes an image in stages:\n",
        "  - **Lower layers** detect simple features like edges.\n",
        "  - **Higher layers** learn more complex patterns and combine features to identify objects.\n",
        "\n",
        "This hierarchical processing mimics how our brain recognizes objects.\n"
      ],
      "metadata": {
        "id": "Ma-m14LfCjwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **What is Stride in CNN?**\n",
        "\n",
        "In Convolutional Neural Networks, **stride** refers to the step size at which the filter (or kernel) moves as it slides over the input image during the convolution operation.\n",
        "\n",
        "- **Stride = 1:** The filter moves one pixel at a time, both horizontally and vertically. This results in a detailed feature map (dense output).\n",
        "- **Stride > 1:** The filter skips over pixels as it moves, which reduces the size of the output feature map, effectively downsampling the input.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Stride Affects the Output Size?**\n",
        "\n",
        "When stride ($S$) is introduced, the output size formula for convolution changes. The modified formula becomes:\n",
        "\n",
        "$$\n",
        "\\text{Output Size} = \\left\\lfloor \\frac{n + 2p - f}{S} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $n$: Size of the input image (e.g., $6 \\times 6$).\n",
        "- $f$: Filter size (e.g., $3 \\times 3$).\n",
        "- $p$: Padding size.\n",
        "- $S$: Stride.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Without Padding ($p = 0$):**\n",
        "\n",
        "- Input image: $6 \\times 6$\n",
        "- Filter: $3 \\times 3$\n",
        "- Stride: $S = 2$\n",
        "\n",
        "Using the formula:\n",
        "$$\n",
        "\\text{Output Size} = \\left\\lfloor \\frac{6 - 3}{2} \\right\\rfloor + 1 = 2 + 1 = 3.\n",
        "$$\n",
        "\n",
        "The output feature map will be $3 \\times 3$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Effect of Stride on Padding Formula:**\n",
        "\n",
        "When $S > 1$, the padding formula must be adjusted to preserve the desired output size (e.g., to keep the same spatial dimensions). The adjusted formula is:\n",
        "\n",
        "$$\n",
        "\\text{Output Size} = \\left\\lfloor \\frac{n + 2p - f}{S} \\right\\rfloor + 1\n",
        "$$\n",
        "\n",
        "For example:\n",
        "- To preserve the input size ($n = 6$), filter size ($f = 3$), stride ($S = 2$), and padding ($p$), we calculate $p$ as:\n",
        "$$\n",
        "p = \\frac{S \\times (\\text{Output Size} - 1) - n + f}{2}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary:**\n",
        "\n",
        "- **Stride** determines how much the filter skips while convolving over the image.\n",
        "- A higher stride reduces the output size, resulting in downsampling.\n",
        "- When $S > 1$, the padding formula needs to account for stride to maintain the desired output size.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ica1CRLCDS4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **How Filter Values are Updated in a Convolutional Neural Network (CNN)**\n",
        "\n",
        "The process of updating **filter values** in a **Convolutional Neural Network (CNN)** is conceptually similar to updating weights in an Artificial Neural Network (ANN). Let's break down the steps involved:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Initializing Filter Values**\n",
        "- Filters (or kernels) are small matrices (e.g., $3 \\times 3$ or $5 \\times 5$) initialized with random values at the start of the training process.\n",
        "- Each filter is designed to learn specific features (like edges, corners, or textures) from the input image.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Forward Pass**\n",
        "- The input image is convolved with the filter(s) to produce **feature maps**.\n",
        "- These feature maps are then passed through an **activation function** (e.g., ReLU) that introduces non-linearity.\n",
        "- The results are forwarded through subsequent layers, leading to predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Calculating the Loss**\n",
        "- A **loss function** (e.g., Mean Squared Error, Cross-Entropy Loss) measures the difference between the predicted output and the actual target label.\n",
        "- The loss represents how far the network’s prediction is from the desired output.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Backpropagation to Update Filter Values**\n",
        "The process of updating filter values is similar to how weights are updated in an ANN. The steps are as follows:\n",
        "\n",
        "#### **Step 1: Compute Gradients**\n",
        "- During backpropagation, the loss is propagated backward through the network.\n",
        "- Gradients of the loss with respect to the filter values are computed using the **chain rule of differentiation**.\n",
        "- These gradients indicate how much each filter contributed to the error.\n",
        "\n",
        "#### **Step 2: Update Filter Values**\n",
        "- Filter values are updated using **Gradient Descent** or its variants (e.g., Adam, RMSprop). The update rule is:\n",
        "\n",
        "$$\n",
        "W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial W}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $W_{\\text{old}}$: Current filter values.\n",
        "- $\\eta$: Learning rate (a small constant that controls the step size).\n",
        "- $\\frac{\\partial \\text{Loss}}{\\partial W}$: Gradient of the loss with respect to the filter values.\n",
        "\n",
        "#### **Step 3: Repeat Until Convergence**\n",
        "- If the filter values do not reach an optimal state after one pass, the process is repeated for multiple iterations (epochs).\n",
        "- Over time, filters learn to detect specific features that minimize the loss.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Do Filters Adapt to the Input Image?**\n",
        "- The gradients are computed based on the specific input image (or batch of images).\n",
        "- This ensures that the filters adapt to the patterns in the training data. For example:\n",
        "  - In early layers, filters might learn to detect simple features like edges or corners.\n",
        "  - In deeper layers, filters may learn more complex features, such as shapes or even entire objects.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "1. Filters in CNNs are analogous to weights in ANNs.\n",
        "2. Filters are initialized randomly and updated during training using backpropagation and gradient descent.\n",
        "3. The goal is to adjust filter values so they can optimally extract features from input images to minimize prediction errors.\n",
        "\n",
        "---\n",
        "\n",
        "Would you like further details on activation functions, loss functions, or optimization techniques?\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "laDNTmYTEAyz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lRN-sP1wEnB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Filters vs. Pooling in a Convolutional Neural Network (CNN)**\n",
        "\n",
        "Both **filters** and **pooling** are crucial in a **Convolutional Neural Network (CNN)**, but they serve **different purposes**. Let's break down each of them:\n",
        "\n",
        "---\n",
        "\n",
        "### **Filters in CNN**\n",
        "\n",
        "Filters (or kernels) are essential components of the **convolution operation**.\n",
        "\n",
        "#### **Purpose of Filters:**\n",
        "- Filters are used to **extract features** from the input image.\n",
        "- They detect patterns such as **edges**, **corners**, **textures**, and other features, which become more complex as the CNN deepens.\n",
        "\n",
        "#### **How Filters Work:**\n",
        "- A filter slides over the input image, performing **element-wise multiplication** and summation (this is the **convolution** operation).\n",
        "- This produces a **feature map**, which highlights specific features that the filter is trained to detect.\n",
        "\n",
        "---\n",
        "\n",
        "### **Pooling in CNN**\n",
        "\n",
        "**Pooling** is a separate operation applied **after** the convolution operation. It serves to **reduce the spatial size** of the feature maps.\n",
        "\n",
        "#### **Purpose of Pooling:**\n",
        "- To **reduce dimensionality** and the number of parameters, making the model more efficient.\n",
        "- To make the network **invariant to small translations or distortions** in the input image (helps generalize).\n",
        "- To reduce the **computational complexity** and **prevent overfitting**.\n",
        "\n",
        "#### **How Pooling Works:**\n",
        "- Pooling involves sliding a window (e.g., \\(2 \\times 2\\)) over the feature map and **summarizing** the values in that window.\n",
        "- Common types of pooling:\n",
        "  - **Max Pooling**: Takes the **maximum value** in each window.\n",
        "  - **Average Pooling**: Takes the **average** of all values in each window.\n",
        "\n",
        "For example, consider the following input feature map:\n",
        "\n",
        "\\[\n",
        "\\begin{bmatrix}\n",
        "1 & 3 & 2 & 1 \\\\\n",
        "4 & 6 & 5 & 2 \\\\\n",
        "7 & 8 & 9 & 4 \\\\\n",
        "3 & 2 & 1 & 5\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "After applying \\(2 \\times 2\\) **Max Pooling** (stride = 2), the resulting feature map is:\n",
        "\n",
        "\\[\n",
        "\\begin{bmatrix}\n",
        "6 & 5 \\\\\n",
        "8 & 9\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences Between Filters and Pooling**\n",
        "\n",
        "| **Aspect**          | **Filters**                                      | **Pooling**                                  |\n",
        "|---------------------|-------------------------------------------------|----------------------------------------------|\n",
        "| **Purpose**          | Extract features (edges, textures, etc.)        | Reduce spatial size and retain essential features. |\n",
        "| **Operation**        | Convolution (element-wise multiplication + sum) | Aggregation (max or average of a window)     |\n",
        "| **Output**           | Feature maps with extracted patterns            | Downsampled feature maps                    |\n",
        "| **Learnable?**       | Yes, filter values are updated during training  | No, pooling is a fixed operation            |\n",
        "\n",
        "---\n",
        "\n",
        "### **Are Both Present in CNN?**\n",
        "\n",
        "Yes, both **filters (convolution layers)** and **pooling layers** are present in CNNs, and they **work together**:\n",
        "\n",
        "1. **Filters (Convolution Layers):**\n",
        "   - Extract specific features from the input image.\n",
        "   - Multiple filters are applied to the same input image, each detecting a different feature (e.g., edges, corners).\n",
        "\n",
        "2. **Pooling Layers:**\n",
        "   - Downsample the feature maps produced by the filters.\n",
        "   - Reduce the spatial dimensions and computational load while preserving essential information.\n",
        "\n",
        "---\n",
        "\n",
        "### **Workflow in CNN:**\n",
        "\n",
        "1. Input Image →  \n",
        "2. **Convolution Layer (Filters)**: Extract features →  \n",
        "3. **Activation Function (e.g., ReLU)**: Add non-linearity →  \n",
        "4. **Pooling Layer**: Reduce feature map size →  \n",
        "5. Repeat for multiple layers.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "w3KbDNcSE0z9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rx8HVyrZFRo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Main Purpose of Pooling\n",
        "\n",
        "1. **Feature Extraction:**\n",
        "   - Pooling focuses on retaining the **most prominent or dominant features** from a feature map.\n",
        "   - For example, in an image of three cats, pooling will help preserve key details (like the outline of a cat’s ear or eyes) while reducing less important information (like background noise).\n",
        "\n",
        "2. **Dimensionality Reduction:**\n",
        "   - Pooling reduces the size of the feature map, making the model computationally efficient.\n",
        "   - This reduced size helps prevent overfitting by simplifying the learned patterns.\n",
        "\n",
        "3. **Translation Invariance:**\n",
        "   - Pooling makes the CNN more robust to minor shifts or distortions in the input image. For example, even if a cat’s ear moves slightly, max pooling will still capture the maximum value in that region, preserving the feature.\n",
        "\n",
        "4. **Why Max Pooling?**\n",
        "   - **Max pooling** selects the largest value in the pooling window, assuming the most important feature in that region is represented by the maximum value.\n",
        "   - **Average pooling** computes the average, which smoothens the feature map and is less commonly used for feature extraction.\n",
        "\n",
        "---\n",
        "\n",
        "### Stride in Pooling vs. Convolution\n",
        "\n",
        "Yes, **stride** in pooling works similarly to the stride in convolution. Here’s how:\n",
        "\n",
        "1. **Stride in Convolution:**\n",
        "   - The filter (e.g., $3 \\times 3$) moves across the image in steps defined by the stride value.\n",
        "   - Example: For stride = 1, the filter moves one pixel at a time. For stride = 2, it skips every alternate pixel.\n",
        "\n",
        "2. **Stride in Pooling:**\n",
        "   - The pooling window (e.g., $2 \\times 2$) moves across the feature map in the same manner.\n",
        "   - If **stride = 1**, the pooling operation overlaps and processes every pixel.  \n",
        "   - If **stride = 2**, it skips pixels, reducing the output size more significantly.\n",
        "\n",
        "#### Example of Max Pooling with Stride:\n",
        "- Input Feature Map:\n",
        "  $$\n",
        "  \\begin{bmatrix}\n",
        "  1 & 3 & 2 & 4 \\\\\n",
        "  5 & 6 & 7 & 8 \\\\\n",
        "  9 & 10 & 11 & 12 \\\\\n",
        "  13 & 14 & 15 & 16\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- Pooling Window: $2 \\times 2$, **Stride = 2** (no overlap).  \n",
        "  The pooling operation slides across the feature map, selecting the maximum value in each window:\n",
        "  $$\n",
        "  \\begin{bmatrix}\n",
        "  \\text{Max}(1, 3, 5, 6) & \\text{Max}(2, 4, 7, 8) \\\\\n",
        "  \\text{Max}(9, 10, 13, 14) & \\text{Max}(11, 12, 15, 16)\n",
        "  \\end{bmatrix}\n",
        "  =\n",
        "  \\begin{bmatrix}\n",
        "  6 & 8 \\\\\n",
        "  14 & 16\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- **Output Feature Map:** $2 \\times 2$.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Differences Between Filters and Pooling\n",
        "\n",
        "| **Aspect**          | **Filter (Convolution)**                          | **Pooling**                                 |\n",
        "|----------------------|--------------------------------------------------|---------------------------------------------|\n",
        "| **Purpose**          | Detect specific patterns/features (e.g., edges). | Downsample while retaining important features. |\n",
        "| **Operation**        | Element-wise multiplication and summation.       | Aggregation (max or average in a window).   |\n",
        "| **Stride**           | Moves the filter across the input image.         | Moves the pooling window across the feature map. |\n",
        "| **Learnable?**       | Yes, filter values are updated during training.   | No, pooling uses fixed operations.          |\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **Pooling Layers** do not detect features like filters but help summarize and reduce the feature maps by keeping the most significant values (e.g., max pooling).\n",
        "- **Stride** in pooling works just like in filters, controlling how far the pooling window moves at each step.\n",
        "- Pooling and filters work together in CNNs: filters extract features, and pooling reduces their size while preserving key information.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "exZMid_dFYSA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "isOixC2dIa8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **What is Flattening?**\n",
        "\n",
        "The flattening layer is used to convert the multi-dimensional output from previous layers (like convolutional and pooling layers) into a **1D vector**. This vector can then be used as input for **dense (fully connected) layers** in the network.\n",
        "\n",
        "#### **Example:**\n",
        "- After passing through convolution and pooling layers, we get a feature map. For example, let's say the output from the pooling layer is a $4 \\times 4 \\times 3$ matrix (4x4 spatial dimensions with 3 channels).\n",
        "- The flattening layer takes this $4 \\times 4 \\times 3$ matrix and reshapes it into a $1D$ vector with $48$ elements:\n",
        "  $$\n",
        "  [x_1, x_2, x_3, \\dots, x_{48}]\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### **Purpose of the Flattening Layer:**\n",
        "\n",
        "1. **Connecting Convolutional Layers to Dense Layers:**\n",
        "   - **Convolutional and pooling layers** work to extract **spatial** and **hierarchical features** of the input data (such as edges, textures, shapes).\n",
        "   - The **dense layers** then perform high-level reasoning and decision-making, like classifying objects or regressing values.\n",
        "   - **Flattening** acts as the intermediary, converting these spatial features into a format (1D vector) that the dense layer can process.\n",
        "\n",
        "2. **Preparation for Classification/Regression:**\n",
        "   - In classification tasks, the flattened vector is passed to the dense layers, where it is transformed into a probability distribution over the classes (e.g., \"cat\" or \"dog\").\n",
        "   - For regression, the dense layer could output a continuous value, like the price of an item.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Does Flattening Work?**\n",
        "\n",
        "To illustrate, let's use a simple example:\n",
        "\n",
        "- **Input:** A feature map of size $4 \\times 4 \\times 3$.\n",
        "- **Flattening operation:** The flattening layer simply rearranges the elements of this matrix into a **1D vector**.\n",
        "\n",
        "If the matrix looks like this:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 2 & 3 & 4 \\\\\n",
        "5 & 6 & 7 & 8 \\\\\n",
        "9 & 10 & 11 & 12 \\\\\n",
        "13 & 14 & 15 & 16\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "After flattening, it becomes:\n",
        "$$\n",
        "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
        "$$\n",
        "- This transformation prepares the data for the next steps in the neural network, where the dense layer will perform computations based on the flattened vector.\n",
        "\n",
        "---\n",
        "\n",
        "### **Flattening vs Dense Layer**\n",
        "\n",
        "While the flattening layer reshapes the data, the dense layer does the actual computation and decision-making:\n",
        "\n",
        "| **Aspect**           | **Flattening**                              | **Dense Layer**                              |\n",
        "|----------------------|---------------------------------------------|---------------------------------------------|\n",
        "| **Purpose**           | Reshape multi-dimensional data into 1D.     | Perform computations to make predictions.   |\n",
        "| **Learnable?**        | No, it’s just a reshaping operation.         | Yes, weights and biases are updated during training. |\n",
        "| **Role in CNN**       | Bridge between convolutional and dense layers. | Final layer(s) for decision-making.         |\n",
        "\n",
        "#### **Dense Layer (Fully Connected Layer):**\n",
        "- After flattening, the **dense layer** uses the 1D vector as input and performs **linear transformations** followed by activation functions (e.g., ReLU, softmax).\n",
        "- The output of a dense layer is computed as:\n",
        "  $$\n",
        "  y = W \\cdot x + b\n",
        "  $$\n",
        "  where:\n",
        "  - $W$ is the weight matrix,\n",
        "  - $x$ is the input vector (the flattened data),\n",
        "  - $b$ is the bias vector.\n",
        "\n",
        "---\n",
        "\n",
        "### **Workflow Example in CNN:**\n",
        "\n",
        "The following steps demonstrate a typical workflow in a Convolutional Neural Network (CNN):\n",
        "\n",
        "1. **Input Image:** An image is passed to the network.\n",
        "2. **Convolution + Pooling Layers:** Extract relevant features from the image (such as edges, textures).\n",
        "3. **Flattening Layer:** Converts the multi-dimensional feature map into a 1D vector.\n",
        "4. **Dense Layers:** Perform classification (e.g., \"cat\" or \"dog\") or regression (e.g., predicting a value).\n",
        "\n",
        "---\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "Let’s assume you are working on a simple **image classification** task with the CNN:\n",
        "1. **Input Image:** A $32 \\times 32 \\times 3$ image.\n",
        "2. **Convolution + Pooling:** The network extracts features and reduces the size of the feature map.\n",
        "3. **Flattening Layer:** The $16 \\times 16 \\times 8$ feature map is flattened into a vector of size $2048$.\n",
        "4. **Dense Layer:** This vector is passed to a dense layer to output a probability distribution across classes.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-UTgTmYOJgg2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gaDayIZJJj3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let's take an example of a small $6 \\times 6$ grayscale image and walk through the steps of convolution, pooling, flattening, and connecting to dense layers. Here's the pixel data of the input image (values range from 0 to 255):\n",
        "\n",
        "### **Input Image (6x6):**\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "34 & 78 & 120 & 200 & 180 & 90 \\\\\n",
        "65 & 90 & 150 & 210 & 190 & 80 \\\\\n",
        "100 & 130 & 170 & 220 & 200 & 100 \\\\\n",
        "95 & 120 & 160 & 215 & 185 & 85 \\\\\n",
        "75 & 100 & 145 & 195 & 175 & 70 \\\\\n",
        "60 & 80 & 110 & 180 & 165 & 55\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Normalize Pixel Values**\n",
        "Normalize each pixel value to a range of 0 to 1 by dividing by 255.  \n",
        "For example, the normalized value of 34 is $ \\frac{34}{255} \\approx 0.133 $.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "0.133 & 0.306 & 0.471 & 0.784 & 0.706 & 0.353 \\\\\n",
        "0.255 & 0.353 & 0.588 & 0.824 & 0.745 & 0.314 \\\\\n",
        "0.392 & 0.510 & 0.667 & 0.863 & 0.784 & 0.392 \\\\\n",
        "0.373 & 0.471 & 0.627 & 0.843 & 0.725 & 0.333 \\\\\n",
        "0.294 & 0.392 & 0.569 & 0.765 & 0.686 & 0.275 \\\\\n",
        "0.235 & 0.314 & 0.431 & 0.706 & 0.647 & 0.216\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Convolution**\n",
        "Use a $3 \\times 3$ filter with the following weights:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & -1 \\\\\n",
        "1 & 0 & -1 \\\\\n",
        "1 & 0 & -1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Stride = 1, no padding.\n",
        "\n",
        "#### Apply Convolution:\n",
        "1. Place the filter on the top-left $3 \\times 3$ region of the normalized image:\n",
        "   $$\n",
        "   \\begin{bmatrix}\n",
        "   0.133 & 0.306 & 0.471 \\\\\n",
        "   0.255 & 0.353 & 0.588 \\\\\n",
        "   0.392 & 0.510 & 0.667\n",
        "   \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "2. Multiply element-wise with the filter and sum:\n",
        "   $$\n",
        "   (0.133 \\cdot 1) + (0.306 \\cdot 0) + (0.471 \\cdot -1) +\n",
        "   (0.255 \\cdot 1) + (0.353 \\cdot 0) + (0.588 \\cdot -1) +\n",
        "   (0.392 \\cdot 1) + (0.510 \\cdot 0) + (0.667 \\cdot -1)\n",
        "   $$\n",
        "   Result: $0.133 - 0.471 + 0.255 - 0.588 + 0.392 - 0.667 = -0.946$.\n",
        "\n",
        "3. Repeat for each $3 \\times 3$ region. Final output (feature map) size is $4 \\times 4$ (using formula $n - f + 1$):\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "-0.946 & \\dots & \\dots & \\dots \\\\\n",
        "\\dots & \\dots & \\dots & \\dots \\\\\n",
        "\\dots & \\dots & \\dots & \\dots \\\\\n",
        "\\dots & \\dots & \\dots & \\dots\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Pooling**\n",
        "Apply $2 \\times 2$ Max Pooling (stride = 2) on the feature map.\n",
        "\n",
        "1. Take each $2 \\times 2$ block and find the maximum value.\n",
        "2. Result: Output size is $2 \\times 2$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Flattening**\n",
        "Flatten the $2 \\times 2$ matrix into a 1D vector:\n",
        "$$\n",
        "[x_1, x_2, x_3, x_4]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Dense Layer**\n",
        "Connect the flattened vector to a dense layer, where each value contributes to the final output through weights and biases.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JWw6-QU4KdiU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_2WbjTJLEDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Step 1: Pooling**\n",
        "We take the **feature map** after convolution and apply $2 \\times 2$ max pooling with a stride of 2.\n",
        "\n",
        "Assume the **feature map after convolution** (from Step 2 in the previous example) is as follows:\n",
        "\n",
        "$$\n",
        "\\text{Feature Map (4x4):}\n",
        "\\begin{bmatrix}\n",
        "-0.946 & -0.371 & 0.215 & 0.489 \\\\\n",
        "0.158 & 0.345 & 0.678 & 0.789 \\\\\n",
        "0.527 & 0.391 & 0.812 & 0.923 \\\\\n",
        "0.234 & 0.412 & 0.653 & 0.789\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### **Max Pooling (2x2, stride=2):**\n",
        "We apply $2 \\times 2$ pooling, taking the **maximum value** in each $2 \\times 2$ block.\n",
        "\n",
        "1. **Top-left block:**\n",
        "   $$\n",
        "   \\begin{bmatrix}\n",
        "   -0.946 & -0.371 \\\\\n",
        "   0.158 & 0.345\n",
        "   \\end{bmatrix}\n",
        "   $$\n",
        "   Maximum = $0.345$.\n",
        "\n",
        "2. **Top-right block:**\n",
        "   $$\n",
        "   \\begin{bmatrix}\n",
        "   0.215 & 0.489 \\\\\n",
        "   0.678 & 0.789\n",
        "   \\end{bmatrix}\n",
        "   $$\n",
        "   Maximum = $0.789$.\n",
        "\n",
        "3. **Bottom-left block:**\n",
        "   $$\n",
        "   \\begin{bmatrix}\n",
        "   0.527 & 0.391 \\\\\n",
        "   0.234 & 0.412\n",
        "   \\end{bmatrix}\n",
        "   $$\n",
        "   Maximum = $0.527$.\n",
        "\n",
        "4. **Bottom-right block:**\n",
        "   $$\n",
        "   \\begin{bmatrix}\n",
        "   0.812 & 0.923 \\\\\n",
        "   0.653 & 0.789\n",
        "   \\end{bmatrix}\n",
        "   $$\n",
        "   Maximum = $0.923$.\n",
        "\n",
        "#### **Output After Pooling (2x2):**\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "0.345 & 0.789 \\\\\n",
        "0.527 & 0.923\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Flattening**\n",
        "We convert the $2 \\times 2$ matrix into a **1D vector** by unraveling the matrix row by row.\n",
        "\n",
        "1. Start with the first row: $[0.345, 0.789]$.\n",
        "2. Add the second row: $[0.527, 0.923]$.\n",
        "\n",
        "#### **Flattened Vector:**\n",
        "$$\n",
        "[0.345, 0.789, 0.527, 0.923]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Connect to Dense Layer**\n",
        "The flattened vector becomes the input for the dense layer. Each value in the vector will be multiplied by a weight, added to a bias, and passed through an activation function.\n",
        "\n",
        "For example:\n",
        "$$\n",
        "\\text{Dense Layer Input: } [0.345, 0.789, 0.527, 0.923]\n",
        "$$\n",
        "This vector will be processed by the dense layer for the final output (e.g., classification probabilities).\n",
        "\n",
        "Let me know if you'd like me to calculate the dense layer output explicitly!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YEkGHU8MOF8p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AReL9qXpOflC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}